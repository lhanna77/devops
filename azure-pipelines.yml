trigger:
  - main

pool:
  vmImage: ubuntu-latest

variables:
  DOCKER_HUB_REPO: lhanna12/spark-job
  IMAGE_TAG: latest

stages:
  - stage: Build
    displayName: "Build and Push Docker Image"
    jobs:
      - job: BuildAndPush
        displayName: "Build & Push Docker Image"
        steps:
          - task: Docker@2
            displayName: "Login to Docker Hub"
            inputs:
              command: login
              containerRegistry: DockerHubServiceConnection

          - task: Docker@2
            displayName: "Build Docker Image"
            inputs:
              command: build
              dockerfile: "Dockerfile"
              repository: "$(DOCKER_HUB_REPO)"
              tags: "$(IMAGE_TAG)"

          - task: Docker@2
            displayName: "Push Docker Image"
            inputs:
              command: push
              repository: "$(DOCKER_HUB_REPO)"
              tags: "$(IMAGE_TAG)"

  - stage: Deploy
    displayName: "Deploy and Run Spark Job"
    dependsOn: Build
    jobs:
      - job: DeploySparkJob
        displayName: "Run Spark Job in Docker"
        steps:
          - script: |
              mkdir -p /app/output
              docker run --name lhanna12-spark-container --network=host \
              -v $(Build.ArtifactStagingDirectory):/app/output \
              -e BUILD_ARTIFACTSTAGINGDIRECTORY=/app/output \
              $(DOCKER_HUB_REPO):$(IMAGE_TAG)
            displayName: "Run Spark Job"
            continueOnError: true

          - script: |
              docker cp lhanna12-spark-container:/app/junit.xml $(System.DefaultWorkingDirectory)/junit.xml
            displayName: "Copy JUnit XML"

          - script: |
              ls $(System.DefaultWorkingDirectory)
            displayName: "List app directory"

          - script: |
              ls $(Build.ArtifactStagingDirectory)
            displayName: "List csv Artifact directory"

          - task: PublishTestResults@2
            inputs:
              testResultsFormat: 'JUnit'
              testResultsFiles: '$(System.DefaultWorkingDirectory)/junit.xml'
            displayName: "Publish PySpark Test Results"
            condition: succeededOrFailed()

          - task: PublishPipelineArtifact@1
            inputs:
              targetPath: '$(System.DefaultWorkingDirectory)/junit.xml'
              artifactName: 'TestResults'
              publishLocation: 'pipeline'
            displayName: 'Publish Test Results as Artifact'

          - task: PublishBuildArtifacts@1
            inputs:
              pathToPublish: "$(Build.ArtifactStagingDirectory)"
              artifactName: "CsvResults"
            displayName: "Publish CSV Results as Artifact"